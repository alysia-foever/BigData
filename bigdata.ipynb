{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d6cfff2-b1fe-41fd-bd3a-4fc803aa56d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypinyin import pinyin, Style\n",
    "from pack.four_corner_method import FourCornerMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a2edb16-6ba0-43c7-a839-24f9d3109844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChineseCharacterCoder:\n",
    "    def __init__(self):\n",
    "        # 初始化字典\n",
    "        self.structure_dict = {}\n",
    "        self.strokes_dict = {\n",
    "            '1':'1', '2':'2', '3':'3', '4':'4', '5':'5', '6':'6', '7':'7', '8':'8', '9':'9', '10':'A',\n",
    "            '11':'B', '12':'C', '13':'D', '14':'E', '15':'F', '16':'G', '17':'H', '18':'I', '19':'J', '20':'K',\n",
    "            '21':'L', '22':'M', '23':'N', '24':'O', '25':'P', '26':'Q', '27':'R', '28':'S', '29':'T', '30':'U',\n",
    "            '31':'V', '32':'W', '33':'X', '34':'Y', '35':'Z', '36':'a', '37':'b', '38':'c', '39':'d', '40':'e',\n",
    "            '41':'f', '42':'g', '43':'h', '44':'i', '45':'j', '46':'k', '47':'l', '48':'m', '49':'n', '50':'o',\n",
    "            '51':'p'}\n",
    "\n",
    "        # 加载汉字结构对照文件\n",
    "        with open('高阶数据集/hanzijiegou_2w.txt', 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    structure, chinese_character = parts\n",
    "                    self.structure_dict[chinese_character] = structure\n",
    "    \n",
    "        # 加载汉字笔画对照文件，参考同级目录下的 chinese_unicode_table.txt 文件格式\n",
    "        self.chinese_char_map = {}\n",
    "        with open('高阶数据集/chinese_unicode_table.txt', 'r', encoding='UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines[6:]: # 前 6 行是表头，去掉\n",
    "                line_info = line.strip().split()\n",
    "                # 处理后的数组第一个是文字，第 7 个是笔画数量\n",
    "                self.chinese_char_map[line_info[0]] = self.strokes_dict[line_info[6]]\n",
    "    def split_pinyin(self, chinese_character): \n",
    "        # 将汉字转换为拼音(带声调) \n",
    "        pinyin_result = pinyin(chinese_character, style=Style.TONE3, heteronym=True)\n",
    "        \n",
    "        # 多音字的话，选择第一个拼音\n",
    "        if pinyin_result: \n",
    "            py = pinyin_result[0][0] \n",
    "        \n",
    "        initials = \"\" # 声母\n",
    "        finals = \"\" # 韵母\n",
    "        codas = \"\" # 补码\n",
    "        tone = \"\" # 声调\n",
    "        \n",
    "        # 声母列表\n",
    "        initials_list = [\"b\", \"p\", \"m\", \"f\", \"d\", \"t\", \"n\", \"l\", \"g\", \"k\", \"h\", \"j\", \"q\", \"x\", \"zh\", \"ch\", \"sh\", \"r\", \"z\", \"c\", \"s\", \"y\", \"w\"] \n",
    "        \n",
    "        # 韵母列表\n",
    "        finals_list = [\"a\", \"o\", \"e\", \"i\", \"u\", \"ü\", \"ai\", \"ei\", \"ui\", \"ao\", \"ou\", \"iu\", \"ie\", \"üe\", \"er\", \"an\", \"en\", \"in\", \"un\", \"ün\", \"ang\", \"eng\", \"ing\", \"ong\"] \n",
    "        \n",
    "        # 获取声调\n",
    "        if py[-1].isdigit(): \n",
    "            tone = py[-1] \n",
    "            py = py[:-1] \n",
    "        \n",
    "        # 获取声母\n",
    "        for initial in initials_list: \n",
    "            if py.startswith(initial): \n",
    "                initials = initial \n",
    "                py = py[len(initial):] \n",
    "                break\n",
    "        \n",
    "        # 获取韵母\n",
    "        for final in finals_list: \n",
    "            if py.endswith(final): \n",
    "                finals = final \n",
    "                py = py[:-len(final)] \n",
    "                break\n",
    "        \n",
    "        # 获取补码\n",
    "        codas = py \n",
    "        \n",
    "        return initials, finals, codas, tone \n",
    "        \n",
    "        return None\n",
    "    def generate_pronunciation_code(self, hanzi): \n",
    "        initial, final, coda, tone = self.split_pinyin(hanzi)\n",
    "        \n",
    "        # 轻声字，例如'了' \n",
    "        if tone == '': \n",
    "            tone = '0' \n",
    "        \n",
    "        # 声母映射\n",
    "        initials_mapping = {'b': '1', 'p': '2', 'm': '3', 'f': '4', 'd': '5', 't': '6', 'n': '7', 'l': '8', \n",
    "            'g': '9', 'k': 'a', 'h': 'b', 'j': 'c', 'q': 'd', 'x': 'e', 'zh': 'f', 'ch': 'g', \n",
    "            'sh': 'h', 'r': 'i', 'z': 'j', 'c': 'k', 's': 'l', 'y': 'm', 'w': 'n'} \n",
    "        \n",
    "        # 韵母映射\n",
    "        finals_mapping = {'a': '1', 'o': '2', 'e': '3', 'i': '4', 'u': '5', 'ü': '6', 'ai': '7', 'ei': '8', \n",
    "            'ui': '9', 'ao': 'a', 'ou': 'b', 'iu': 'c', 'ie': 'd', 'üe': 'e', 'er': 'f', \n",
    "            'an': 'g', 'en': 'h', 'in': 'i', 'un': 'j', 'ün': 'k', 'ang': 'l', 'eng': 'm', \n",
    "            'ing': 'n', 'ong': 'o'} \n",
    "        \n",
    "        # 补码映射\n",
    "        coda_mapping = {'': '0', 'u':'1', 'i':'1'} \n",
    "        \n",
    "        # 获取映射值\n",
    "        initial_code = initials_mapping.get(initial, '0')\n",
    "        final_code = finals_mapping.get(final, '0')\n",
    "        coda_code = coda_mapping.get(coda, '0')\n",
    "        \n",
    "        # 组合生成四位数的字音编码\n",
    "        pronunciation_code = initial_code + final_code + coda_code + tone \n",
    "        \n",
    "        return pronunciation_code\n",
    "    def generate_glyph_code(self, hanzi): \n",
    "        # 获取汉字的结构\n",
    "        structure_code = self.structure_dict[hanzi] \n",
    "        \n",
    "        # 获取汉字的四角编码\n",
    "        fcc = FourCornerMethod().query(hanzi)\n",
    "        \n",
    "        # 获取汉字的笔画数\n",
    "        stroke = self.chinese_char_map[hanzi] \n",
    "        \n",
    "        # 组合生成的字形编码\n",
    "        glyph_code = structure_code + fcc + stroke \n",
    "        \n",
    "        return glyph_code\n",
    "    def generate_character_code(self, hanzi):\n",
    "        return self.generate_glyph_code(hanzi) + self.generate_pronunciation_code(hanzi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba5925-672c-435b-b647-8d9fa64877aa",
   "metadata": {},
   "source": [
    "# 3.构建字符相似性网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66e9878e-6d3b-45b4-856e-9d0b5ec2980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建字符相似性网络（用矩阵形式表示）\n",
    "def compute_sim_mat(chinese_characters, chinese_characters_count, chinese_characters_code):\n",
    "    sim_mat = [[0] * len(chinese_characters) for _ in range(len(chinese_characters))]\n",
    "    \n",
    "    for i in tqdm(range(len(chinese_characters)), desc='Constructing Similarity Matrix', unit='i'):\n",
    "        for j in range(i, len(chinese_characters)):\n",
    "            similarity = computeSSCsimilarity(\n",
    "                chinese_characters_code[chinese_characters[i]],\n",
    "                chinese_characters_code[chinese_characters[j]]\n",
    "            )\n",
    "            sim_mat[i][j] = similarity\n",
    "            sim_mat[j][i] = similarity\n",
    "\n",
    "    # 将结果写入文件\n",
    "    output_file = 'similarity_matrix.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for row in sim_mat:\n",
    "            f.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "    return sim_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbf273-bc6d-4841-96f6-8e1710ba97cb",
   "metadata": {},
   "source": [
    "# 4.利用字符相似性网络进行字符嵌入学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b664fd05-e464-421e-9b44-953f83b0cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据字符相似性网络生成最终的字嵌入向量\n",
    "def generate_char_vectors(chinese_characters, w2v_vectors, sim_mat, text, chinese_characters_count, threshold=0.6):\n",
    "    char_vectors = {}\n",
    "    for i in tqdm(range(len(chinese_characters)), desc='Generating char vectors'):\n",
    "        character = chinese_characters[i]\n",
    "        similar_group = []\n",
    "        for j in range(len(sim_mat[i])):\n",
    "            if sim_mat[i][j] >= threshold:\n",
    "                similar_group.append(chinese_characters[j])\n",
    "        sum_count = 0\n",
    "        emb = np.zeros_like(w2v_vectors[list(w2v_vectors.keys())[0]])  # 初始化一个全零向量\n",
    "        for c in similar_group:\n",
    "            if c not in w2v_vectors.keys():\n",
    "                update(w2v_vectors, text, c)\n",
    "            emb += chinese_characters_count[c] * w2v_vectors[c]\n",
    "            sum_count += chinese_characters_count[c]\n",
    "        emb /= sum_count if sum_count else 1  # 避免除以0\n",
    "        char_vectors[character] = emb\n",
    "\n",
    "    return char_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1809b-1123-4d4f-836d-2f3fa32a87fd",
   "metadata": {},
   "source": [
    "# 5.生成句子嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0377edb9-b716-4d89-ab0a-b861821c9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据字嵌入向量生成句子嵌入向量\n",
    "def generate_sentence_vectors(texts, char_vectors, d=100):\n",
    "    sentence_vectors = []\n",
    "    for text in tqdm(texts, desc='Generating sentence vectors'):\n",
    "        alpha = np.zeros((len(text), len(text)))\n",
    "        for i in range(len(text)):\n",
    "            for j in range(len(text)):\n",
    "                alpha[i][j] = alpha[i][j] = np.dot(char_vectors[text[i]], char_vectors[text[j]]) / np.sqrt(d)\n",
    "\n",
    "        alpha_hat = np.zeros_like(alpha)\n",
    "        for i in range(len(text)):\n",
    "            for j in range(len(text)):\n",
    "                alpha_hat[i][j] = alpha_hat[i][j] = np.exp(alpha[i][j]) / np.sum(alpha[i])\n",
    "\n",
    "        m = np.zeros((d,))  # 初始化一个全零向量\n",
    "        for i in range(len(text)):\n",
    "            mi = np.zeros((d,))\n",
    "            for j in range(len(text)):\n",
    "                mi += alpha_hat[i][j] * char_vectors[text[j]]\n",
    "            m += mi\n",
    "        sentence_vectors.append(m / d)\n",
    "\n",
    "    return sentence_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ff335-56dd-4102-b275-04f0dc0cfd2a",
   "metadata": {},
   "source": [
    "# 6.构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e293b67-6a25-4029-ae08-3593bcf36e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 垃圾文本分类\n",
    "def spam_classification(train_tags, train_word_vectors, test_tags, test_word_vectors):\n",
    "    # 使用逻辑回归模型\n",
    "    logistic_repression = LogisticRegression()\n",
    "    logistic_repression.fit(np.array(train_word_vectors), np.array(train_tags))\n",
    "    predictions = logistic_repression.predict(test_word_vectors)\n",
    "\n",
    "    # 输出混淆矩阵和分类报告\n",
    "    cm = confusion_matrix(np.array(test_tags), np.array(predictions))\n",
    "    print(\"混淆矩阵:\")\n",
    "    print(cm)\n",
    "\n",
    "    report = classification_report(np.array(test_tags), np.array(predictions))\n",
    "    print(\"分类报告:\")\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06ab09-ae09-4bb4-8076-e7a3b767b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134caced-cfc0-4d6e-9701-dd039a905a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
