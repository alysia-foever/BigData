{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6cfff2-b1fe-41fd-bd3a-4fc803aa56d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypinyin import pinyin, Style\n",
    "from pack.four_corner_method import FourCornerMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2edb16-6ba0-43c7-a839-24f9d3109844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChineseCharacterCoder:\n",
    "    def __init__(self):\n",
    "        # 初始化字典\n",
    "        self.structure_dict = {}\n",
    "        self.strokes_dict = {\n",
    "            '1':'1', '2':'2', '3':'3', '4':'4', '5':'5', '6':'6', '7':'7', '8':'8', '9':'9', '10':'A',\n",
    "            '11':'B', '12':'C', '13':'D', '14':'E', '15':'F', '16':'G', '17':'H', '18':'I', '19':'J', '20':'K',\n",
    "            '21':'L', '22':'M', '23':'N', '24':'O', '25':'P', '26':'Q', '27':'R', '28':'S', '29':'T', '30':'U',\n",
    "            '31':'V', '32':'W', '33':'X', '34':'Y', '35':'Z', '36':'a', '37':'b', '38':'c', '39':'d', '40':'e',\n",
    "            '41':'f', '42':'g', '43':'h', '44':'i', '45':'j', '46':'k', '47':'l', '48':'m', '49':'n', '50':'o',\n",
    "            '51':'p'}\n",
    "\n",
    "        # 加载汉字结构对照文件\n",
    "        with open('高阶数据集/hanzijiegou_2w.txt', 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    structure, chinese_character = parts\n",
    "                    self.structure_dict[chinese_character] = structure\n",
    "    \n",
    "        # 加载汉字笔画对照文件，参考同级目录下的 chinese_unicode_table.txt 文件格式\n",
    "        self.chinese_char_map = {}\n",
    "        with open('高阶数据集/chinese_unicode_table.txt', 'r', encoding='UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines[6:]: # 前 6 行是表头，去掉\n",
    "                line_info = line.strip().split()\n",
    "                # 处理后的数组第一个是文字，第 7 个是笔画数量\n",
    "                self.chinese_char_map[line_info[0]] = self.strokes_dict[line_info[6]]\n",
    "    def split_pinyin(self, chinese_character): \n",
    "        # 将汉字转换为拼音(带声调) \n",
    "        pinyin_result = pinyin(chinese_character, style=Style.TONE3, heteronym=True)\n",
    "        \n",
    "        # 多音字的话，选择第一个拼音\n",
    "        if pinyin_result: \n",
    "            py = pinyin_result[0][0] \n",
    "        \n",
    "        initials = \"\" # 声母\n",
    "        finals = \"\" # 韵母\n",
    "        codas = \"\" # 补码\n",
    "        tone = \"\" # 声调\n",
    "        \n",
    "        # 声母列表\n",
    "        initials_list = [\"b\", \"p\", \"m\", \"f\", \"d\", \"t\", \"n\", \"l\", \"g\", \"k\", \"h\", \"j\", \"q\", \"x\", \"zh\", \"ch\", \"sh\", \"r\", \"z\", \"c\", \"s\", \"y\", \"w\"] \n",
    "        \n",
    "        # 韵母列表\n",
    "        finals_list = [\"a\", \"o\", \"e\", \"i\", \"u\", \"ü\", \"ai\", \"ei\", \"ui\", \"ao\", \"ou\", \"iu\", \"ie\", \"üe\", \"er\", \"an\", \"en\", \"in\", \"un\", \"ün\", \"ang\", \"eng\", \"ing\", \"ong\"] \n",
    "        \n",
    "        # 获取声调\n",
    "        if py[-1].isdigit(): \n",
    "            tone = py[-1] \n",
    "            py = py[:-1] \n",
    "        \n",
    "        # 获取声母\n",
    "        for initial in initials_list: \n",
    "            if py.startswith(initial): \n",
    "                initials = initial \n",
    "                py = py[len(initial):] \n",
    "                break\n",
    "        \n",
    "        # 获取韵母\n",
    "        for final in finals_list: \n",
    "            if py.endswith(final): \n",
    "                finals = final \n",
    "                py = py[:-len(final)] \n",
    "                break\n",
    "        \n",
    "        # 获取补码\n",
    "        codas = py \n",
    "        \n",
    "        return initials, finals, codas, tone \n",
    "        \n",
    "        return None\n",
    "    def generate_pronunciation_code(self, hanzi): \n",
    "        initial, final, coda, tone = self.split_pinyin(hanzi)\n",
    "        \n",
    "        # 轻声字，例如'了' \n",
    "        if tone == '': \n",
    "            tone = '0' \n",
    "        \n",
    "        # 声母映射\n",
    "        initials_mapping = {'b': '1', 'p': '2', 'm': '3', 'f': '4', 'd': '5', 't': '6', 'n': '7', 'l': '8', \n",
    "            'g': '9', 'k': 'a', 'h': 'b', 'j': 'c', 'q': 'd', 'x': 'e', 'zh': 'f', 'ch': 'g', \n",
    "            'sh': 'h', 'r': 'i', 'z': 'j', 'c': 'k', 's': 'l', 'y': 'm', 'w': 'n'} \n",
    "        \n",
    "        # 韵母映射\n",
    "        finals_mapping = {'a': '1', 'o': '2', 'e': '3', 'i': '4', 'u': '5', 'ü': '6', 'ai': '7', 'ei': '8', \n",
    "            'ui': '9', 'ao': 'a', 'ou': 'b', 'iu': 'c', 'ie': 'd', 'üe': 'e', 'er': 'f', \n",
    "            'an': 'g', 'en': 'h', 'in': 'i', 'un': 'j', 'ün': 'k', 'ang': 'l', 'eng': 'm', \n",
    "            'ing': 'n', 'ong': 'o'} \n",
    "        \n",
    "        # 补码映射\n",
    "        coda_mapping = {'': '0', 'u':'1', 'i':'1'} \n",
    "        \n",
    "        # 获取映射值\n",
    "        initial_code = initials_mapping.get(initial, '0')\n",
    "        final_code = finals_mapping.get(final, '0')\n",
    "        coda_code = coda_mapping.get(coda, '0')\n",
    "        \n",
    "        # 组合生成四位数的字音编码\n",
    "        pronunciation_code = initial_code + final_code + coda_code + tone \n",
    "        \n",
    "        return pronunciation_code\n",
    "    def generate_glyph_code(self, hanzi): \n",
    "        # 获取汉字的结构\n",
    "        structure_code = self.structure_dict[hanzi] \n",
    "        \n",
    "        # 获取汉字的四角编码\n",
    "        fcc = FourCornerMethod().query(hanzi)\n",
    "        \n",
    "        # 获取汉字的笔画数\n",
    "        stroke = self.chinese_char_map[hanzi] \n",
    "        \n",
    "        # 组合生成的字形编码\n",
    "        glyph_code = structure_code + fcc + stroke \n",
    "        \n",
    "        return glyph_code\n",
    "    def generate_character_code(self, hanzi):\n",
    "        return self.generate_glyph_code(hanzi) + self.generate_pronunciation_code(hanzi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba5925-672c-435b-b647-8d9fa64877aa",
   "metadata": {},
   "source": [
    "# 3.构建字符相似性网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66e9878e-6d3b-45b4-856e-9d0b5ec2980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建字符相似性网络（用矩阵形式表示）\n",
    "def compute_sim_mat(chinese_characters, chinese_characters_count, chinese_characters_code):\n",
    "    sim_mat = [[0] * len(chinese_characters) for _ in range(len(chinese_characters))]\n",
    "    \n",
    "    for i in tqdm(range(len(chinese_characters)), desc='Constructing Similarity Matrix', unit='i'):\n",
    "        for j in range(i, len(chinese_characters)):\n",
    "            similarity = computeSSCsimilarity(\n",
    "                chinese_characters_code[chinese_characters[i]],\n",
    "                chinese_characters_code[chinese_characters[j]]\n",
    "            )\n",
    "            sim_mat[i][j] = similarity\n",
    "            sim_mat[j][i] = similarity\n",
    "\n",
    "    # 将结果写入文件\n",
    "    output_file = 'similarity_matrix.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for row in sim_mat:\n",
    "            f.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "    return sim_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbf273-bc6d-4841-96f6-8e1710ba97cb",
   "metadata": {},
   "source": [
    "# 4.利用字符相似性网络进行字符嵌入学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b664fd05-e464-421e-9b44-953f83b0cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据字符相似性网络生成最终的字嵌入向量\n",
    "def generate_char_vectors(chinese_characters, w2v_vectors, sim_mat, text, chinese_characters_count, threshold=0.6):\n",
    "    char_vectors = {}\n",
    "    for i in tqdm(range(len(chinese_characters)), desc='Generating char vectors'):\n",
    "        character = chinese_characters[i]\n",
    "        similar_group = []\n",
    "        for j in range(len(sim_mat[i])):\n",
    "            if sim_mat[i][j] >= threshold:\n",
    "                similar_group.append(chinese_characters[j])\n",
    "        sum_count = 0\n",
    "        emb = np.zeros_like(w2v_vectors[list(w2v_vectors.keys())[0]])  # 初始化一个全零向量\n",
    "        for c in similar_group:\n",
    "            if c not in w2v_vectors.keys():\n",
    "                update(w2v_vectors, text, c)\n",
    "            emb += chinese_characters_count[c] * w2v_vectors[c]\n",
    "            sum_count += chinese_characters_count[c]\n",
    "        emb /= sum_count if sum_count else 1  # 避免除以0\n",
    "        char_vectors[character] = emb\n",
    "\n",
    "    return char_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1809b-1123-4d4f-836d-2f3fa32a87fd",
   "metadata": {},
   "source": [
    "# 5.生成句子嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0377edb9-b716-4d89-ab0a-b861821c9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据字嵌入向量生成句子嵌入向量\n",
    "def generate_sentence_vectors(texts, char_vectors, d=100):\n",
    "    sentence_vectors = []\n",
    "    for text in tqdm(texts, desc='Generating sentence vectors'):\n",
    "        alpha = np.zeros((len(text), len(text)))\n",
    "        for i in range(len(text)):\n",
    "            for j in range(len(text)):\n",
    "                alpha[i][j] = alpha[i][j] = np.dot(char_vectors[text[i]], char_vectors[text[j]]) / np.sqrt(d)\n",
    "\n",
    "        alpha_hat = np.zeros_like(alpha)\n",
    "        for i in range(len(text)):\n",
    "            for j in range(len(text)):\n",
    "                alpha_hat[i][j] = alpha_hat[i][j] = np.exp(alpha[i][j]) / np.sum(alpha[i])\n",
    "\n",
    "        m = np.zeros((d,))  # 初始化一个全零向量\n",
    "        for i in range(len(text)):\n",
    "            mi = np.zeros((d,))\n",
    "            for j in range(len(text)):\n",
    "                mi += alpha_hat[i][j] * char_vectors[text[j]]\n",
    "            m += mi\n",
    "        sentence_vectors.append(m / d)\n",
    "\n",
    "    return sentence_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ff335-56dd-4102-b275-04f0dc0cfd2a",
   "metadata": {},
   "source": [
    "# 6.构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e293b67-6a25-4029-ae08-3593bcf36e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 垃圾文本分类\n",
    "def spam_classification(train_tags, train_word_vectors, test_tags, test_word_vectors):\n",
    "    # 使用逻辑回归模型\n",
    "    logistic_repression = LogisticRegression()\n",
    "    logistic_repression.fit(np.array(train_word_vectors), np.array(train_tags))\n",
    "    predictions = logistic_repression.predict(test_word_vectors)\n",
    "\n",
    "    # 输出混淆矩阵和分类报告\n",
    "    cm = confusion_matrix(np.array(test_tags), np.array(predictions))\n",
    "    print(\"混淆矩阵:\")\n",
    "    print(cm)\n",
    "\n",
    "    report = classification_report(np.array(test_tags), np.array(predictions))\n",
    "    print(\"分类报告:\")\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb06ab09-ae09-4bb4-8076-e7a3b767b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在统计文件行数...\n",
      "正在生成随机索引...\n",
      "正在并行处理文件分块...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理进度: 16032it [00:00, 73273.89it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数据集已随机按1:1比例划分为 高阶数据集/train.txt 和 高阶数据集/test.txt\n",
      "训练集行数: 8003\n",
      "测试集行数: 8004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def split_dataset_random_parallel(input_file, train_file, test_file, chunk_size=10000):\n",
    "    # 第一步：读取文件并获取总行数\n",
    "    print(\"正在统计文件行数...\")\n",
    "    with open(input_file, 'r') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "    \n",
    "    # 第二步：生成随机索引\n",
    "    print(\"正在生成随机索引...\")\n",
    "    indices = list(range(total_lines))\n",
    "    random.shuffle(indices)\n",
    "    split_point = total_lines // 2\n",
    "    train_indices = set(indices[:split_point])\n",
    "    \n",
    "    # 第三步：并行处理文件分块\n",
    "    print(\"正在并行处理文件分块...\")\n",
    "    \n",
    "    # 创建输出文件\n",
    "    open(train_file, 'w').close()\n",
    "    open(test_file, 'w').close()\n",
    "    \n",
    "    def process_chunk(chunk_start, chunk_end):\n",
    "        train_lines = []\n",
    "        test_lines = []\n",
    "        with open(input_file, 'r') as f:\n",
    "            # 定位到块起始位置\n",
    "            for _ in range(chunk_start):\n",
    "                next(f)\n",
    "            # 读取块内容\n",
    "            for i in range(chunk_start, min(chunk_end, total_lines)):\n",
    "                line = next(f)\n",
    "                if i in train_indices:\n",
    "                    train_lines.append(line)\n",
    "                else:\n",
    "                    test_lines.append(line)\n",
    "        \n",
    "        # 写入文件\n",
    "        with open(train_file, 'a') as train:\n",
    "            train.writelines(train_lines)\n",
    "        with open(test_file, 'a') as test:\n",
    "            test.writelines(test_lines)\n",
    "    \n",
    "    # 使用线程池并行处理\n",
    "    chunk_size = min(chunk_size, total_lines // os.cpu_count() + 1)\n",
    "    with ThreadPoolExecutor() as executor, tqdm(total=total_lines, desc=\"处理进度\") as pbar:\n",
    "        futures = []\n",
    "        for chunk_start in range(0, total_lines, chunk_size):\n",
    "            chunk_end = chunk_start + chunk_size\n",
    "            future = executor.submit(process_chunk, chunk_start, chunk_end)\n",
    "            future.add_done_callback(lambda _: pbar.update(chunk_size))\n",
    "            futures.append(future)\n",
    "        \n",
    "        # 等待所有任务完成\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "    print(f\"\\n数据集已随机按1:1比例划分为 {train_file} 和 {test_file}\")\n",
    "    print(f\"训练集行数: {split_point}\")\n",
    "    print(f\"测试集行数: {total_lines - split_point}\")\n",
    "\n",
    "# 使用示例\n",
    "split_dataset_random_parallel('高阶数据集/dataset.txt', '高阶数据集/train.txt', '高阶数据集/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "134caced-cfc0-4d6e-9701-dd039a905a3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 读取数据并划分标签和文本\n",
    "def divide_dataset(filename, lines=1000):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        text_data = f.readlines()\n",
    "\n",
    "    # 选择前lines行的数据\n",
    "    subset = text_data[:lines]\n",
    "    # 分离每一行的标签和文本\n",
    "    dataset = [s.strip().split('\\t') for s in subset]\n",
    "    \n",
    "    # 去除空文本项\n",
    "    dataset = [data for data in dataset if len(data) == 2 and data[1].strip()]\n",
    "    \n",
    "    tag = [data[0] for data in dataset]\n",
    "    text = [data[1] for data in dataset]\n",
    "\n",
    "    return tag, text\n",
    "\n",
    "# 文本清洗\n",
    "def clean_text(dataset):\n",
    "    cleaned_text = []\n",
    "    # for text in tqdm(dataset, desc='Cleaning text'):\n",
    "    for text in dataset:\n",
    "        # 仅保留中文字符、字母和数字\n",
    "        clean = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z0-9\\s]', '', text)  \n",
    "        # 处理缺失值和异常值\n",
    "        cleaned_text.append(clean.strip())\n",
    "    return cleaned_text\n",
    "\n",
    "# 文本标记化和停用词处理\n",
    "def tokenize_and_remove_stopwords(dataset):\n",
    "    stopwords_file = '第四章/基本示例/数据集/hit_stopwords.txt'\n",
    "    with open(stopwords_file, 'r', encoding='utf-8') as file:\n",
    "        stopwords = {line.strip() for line in file}\n",
    "\n",
    "    tokenized_text = []\n",
    "    # for text in tqdm(dataset, desc='Tokenizing and removing stopwords'):\n",
    "    for text in dataset:\n",
    "        # 使用jieba进行分词\n",
    "        words = jieba.lcut(text)  \n",
    "        # 移除停用词\n",
    "        filtered_words = [word for word in words if word not in stopwords]  \n",
    "        tokenized_text.append(filtered_words)\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "# 特征提取\n",
    "def generate_text_vectors(tokenized_text):\n",
    "    model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, sg=0)\n",
    "    word_vectors = model.wv\n",
    "\n",
    "    text_vectors = []\n",
    "    # for tokens in tqdm(tokenized_text, desc='Generating text vectors'):\n",
    "    for tokens in tokenized_text:\n",
    "        # 转换为词向量表示\n",
    "        vectors = [word_vectors[word] for word in tokens if word in word_vectors]  \n",
    "        if vectors:\n",
    "            # 取平均值\n",
    "            text_vectors.append(np.mean(vectors, axis=0))  \n",
    "        else:\n",
    "            # 如果没有词向量则用0向量代替\n",
    "            text_vectors.append(np.zeros(100))  \n",
    "\n",
    "    return text_vectors\n",
    "\n",
    "# 垃圾文本分类\n",
    "def spam_classification(train_tags, train_word_vectors, test_tags, test_word_vectors):\n",
    "    # 使用RandomOverSampler进行过采样\n",
    "    #oversampler = RandomOverSampler(sampling_strategy=0.5, random_state=42)\n",
    "    #X_resampled, y_resampled = oversampler.fit_resample(train_word_vectors, train_tags)\n",
    "\n",
    "    # 使用支持向量机分类器\n",
    "    svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "    # 定义参数网格\n",
    "    #param_grid = {\n",
    "    #    'kernel': ['linear', 'rbf'],  # 选择核函数\n",
    "    #    'C': [0.1, 0.5, 1, 5, 10],             # 正则化参数\n",
    "    #}\n",
    "    # 创建GridSearchCV对象\n",
    "    #grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='recall', verbose=2, n_jobs=-1)\n",
    "    # 在过采样后的训练数据上进行网格搜索\n",
    "    #grid_search.fit(train_word_vectors, train_tags)\n",
    "    # 输出最佳参数\n",
    "    #print(\"最佳参数组合:\", grid_search.best_params_)\n",
    "\n",
    "    svm_classifier.fit(np.array(train_word_vectors), np.array(train_tags))  \n",
    "\n",
    "    # 在测试集上进行预测并显示进度条\n",
    "    predictions = []\n",
    "    for vector in tqdm(test_word_vectors, desc='Classifying', leave=False):\n",
    "        prediction = svm_classifier.predict([vector])\n",
    "        #prediction = grid_search.predict([vector])\n",
    "        predictions.append(prediction[0])\n",
    "\n",
    "    # 输出混淆矩阵和分类报告\n",
    "    cm = confusion_matrix(np.array(test_tags), np.array(predictions))\n",
    "    print(\"混淆矩阵:\")\n",
    "    print(cm)\n",
    "\n",
    "    report = classification_report(np.array(test_tags), np.array(predictions))\n",
    "    print(\"分类报告:\")\n",
    "    print(report)\n",
    "\n",
    "    # 输出模型评估结果\n",
    "    # accuracy = accuracy_score(np.array(test_tags), np.array(predictions))\n",
    "    # print(f'准确率: {accuracy:.2f}')\n",
    "\n",
    "train_tags, train_text = divide_dataset(\"高阶数据集/train.txt\", 8003)\n",
    "test_tags, test_text = divide_dataset(\"高阶数据集/test.txt\", 8004)\n",
    "\n",
    "cleaned_train_text = clean_text(train_text)\n",
    "cleaned_test_text = clean_text(test_text)\n",
    "\n",
    "train_tokenized_text = tokenize_and_remove_stopwords(cleaned_train_text)\n",
    "test_tokenized_text = tokenize_and_remove_stopwords(cleaned_test_text)\n",
    "\n",
    "train_word_vectors = generate_text_vectors(train_tokenized_text)\n",
    "test_word_vectors = generate_text_vectors(test_tokenized_text)\n",
    "\n",
    "spam_classification(train_tags, train_word_vectors, test_tags, test_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a69de62-1773-48ce-9e86-ee2c81aad2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
